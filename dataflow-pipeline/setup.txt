export PROJECT=<<put the project id>>
export REGION=<<put the region>>
export STAGING_LOCATION=gs://vitaming-demo/staging/
export TEMP_LOCATION=gs://vitaming-demo/temp/
export TEMPLATE_LOCATION=gs://vitaming-demo/templates/

##Submitting a new pipeline for bigtable
python3 -m streaming-pubsub-to-bigtable --job_name pubsub-dataflow-bigtable --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime

##Update existing pipeline for bigtable
python3 -m streaming-pubsub-to-bigtable --job_name pubsub-dataflow-bigtable --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --update --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime --jobName=logistics-pipeline-bigtable

##Submitting a new pipeline for bigquery
python3 -m streaming-pubsub-to-bq --job_name pubsub-dataflow-bigquery --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime

##Update existing pipeline for bigtable
python3 -m streaming-pubsub-to-bq --job_name pubsub-dataflow-bigquery --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --update --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime


##Submitting a new pipeline for bigquery and bigtable
python3 -m streaming-pubsub-to-bq --job_name pubsub-dataflow-bigquery-bigtable --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime

##Generate the template
python -m streaming-pubsub-to-bq-bigtable     --runner DataflowRunner     --project on-prem-project-337210     --staging_location gs://customer-demos-asia-south1/staging     --temp_location gs://customer-demos-asia-south1/temp     --template_location gs://customer-demos-asia-south1/templates/logistics-streaming-bq-bigtable     --region asia-south1
