export PROJECT=on-prem-project-337210
export REGION=asia-south1
export STAGING_LOCATION=gs://vitaming-demo/staging/
export TEMP_LOCATION=gs://vitaming-demo/temp/
export TEMPLATE_LOCATION=gs://vitaming-demo/templates/

##Submitting a new pipeline for bigtable
python3 -m streaming-pubsub-to-bigtable --job_name pubsub-dataflow-bigtable --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime

##Update existing pipeline for bigtable
python3 -m streaming-pubsub-to-bigtable --job_name pubsub-dataflow-bigtable --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --update --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime --jobName=logistics-pipeline-bigtable

##Submitting a new pipeline for bigquery
python3 -m streaming-pubsub-to-bq --job_name pubsub-dataflow-bigquery --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime

##Update existing pipeline for bigtable
python3 -m streaming-pubsub-to-bq --job_name pubsub-dataflow-bigquery --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --update --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime


##Submitting a new pipeline for bigquery and bigtable
python3 -m streaming-pubsub-to-bq --job_name pubsub-dataflow-bigquery-bigtable --runner DataflowRunner --project=$PROJECT --staging_location=$STAGING_LOCATION --temp_location=$TEMP_LOCATION --region=$REGION --streaming --subnetwork=https://www.googleapis.com/compute/v1/projects/on-prem-project-337210/regions/asia-south1/subnetworks/on-prem-subnet-mumbai --dataflow_service_options=enable_prime